#!/usr/bin/env python3
"""Build comprehensive X API references from docs.x.com/llms.txt.

Outputs:
- references/x-api-catalog.md: all x-api docs links, with OpenAPI endpoint extraction when present
- references/x-api-groups.md: navigation for curated reference files
- references/autogen/<group>.md: full autogenerated inventory (endpoints + docs) per group
- Updates curated group reference files (e.g. references/users.md) to be lean:
  it removes any inlined autogenerated inventory (if present) and links to `references/autogen/<group>.md`.

This script is dependency-free.

Usage:
  python3 build_x_docs_catalog.py [--llms-url URL] [--out-dir DIR]

Defaults:
  --llms-url https://docs.x.com/llms.txt
  --out-dir  references/ (relative to this repo)

Notes:
- We only fetch the first ~256 KiB of each doc page to detect/extract the OpenAPI fenced YAML header
  and the operation-level security block. This is enough because the OpenAPI blocks are at the top.
"""

from __future__ import annotations

import argparse
import concurrent.futures
import dataclasses
import os
import re
import sys
import time
import urllib.request
from collections import Counter, defaultdict
from typing import Dict, Iterable, List, Optional, Tuple


LLMS_DEFAULT = "https://docs.x.com/llms.txt"
FETCH_MAX_BYTES = 256 * 1024
FETCH_TIMEOUT_S = 30
WORKERS = 10
DEFAULT_OUT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "references"))
AUTOGEN_DIRNAME = "autogen"

CURATED_AUTOGEN_BOILERPLATE = (
    "Curated notes should go above. The section below is autogenerated from the X docs index and will be updated by the generator."
)
INVENTORY_HEADING = "## Inventory"
FULL_AUTOGEN_HINT_PREFIX = "Full autogenerated inventory:"
MAX_DOC_LINKS_IN_REFERENCE = 20
MAX_ENDPOINTS_IN_REFERENCE = 30


@dataclasses.dataclass(frozen=True)
class LlmsItem:
    title: str
    url: str


@dataclasses.dataclass(frozen=True)
class OpenApiInfo:
    method: str
    path: str
    server_url: Optional[str]
    security_lines: List[str]


def _read_url(url: str, max_bytes: int) -> str:
    req = urllib.request.Request(
        url,
        headers={
            "User-Agent": "x-api-skill-catalog-builder/1.0",
            "Accept": "text/markdown,text/plain,*/*",
        },
    )
    with urllib.request.urlopen(req, timeout=FETCH_TIMEOUT_S) as resp:
        data = resp.read(max_bytes + 1)
    return data[:max_bytes].decode("utf-8", errors="replace")


def parse_llms(text: str) -> List[LlmsItem]:
    items: List[LlmsItem] = []
    for line in text.splitlines():
        m = re.match(r"^- \[(.*?)\]\((https?://[^)]+)\)", line)
        if not m:
            continue
        url = m.group(2).strip()
        raw_title = m.group(1).strip()
        title = raw_title
        if not title or title.lower() == "null":
            title = _fallback_title_from_url(url)
        items.append(LlmsItem(title=title, url=url))
    return items


def _fallback_title_from_url(url: str) -> str:
    # e.g. https://docs.x.com/x-api/posts/get-post-by-id.md -> "Get Post By Id"
    u = url.split("?", 1)[0].rstrip("/")
    tail = u.rsplit("/", 1)[-1]
    base = tail.rsplit(".", 1)[0]
    cleaned = re.sub(r"[\s_-]+", " ", base).strip()
    return cleaned.title() if cleaned else url


def _render_doc_link(it: LlmsItem) -> str:
    title = (it.title or "").strip()
    if not title or title.lower() == "null":
        title = _fallback_title_from_url(it.url)
    return f"- {title}: `{it.url}`"


def _leading_spaces(s: str) -> int:
    return len(s) - len(s.lstrip(" "))


def try_extract_openapi(md: str) -> Optional[OpenApiInfo]:
    # Find first 4-backtick yaml fence.
    m = re.search(r"(?m)^(`{4})yaml\s+(\w+)\s+([^\s]+)\s*$", md)
    if not m:
        return None

    fence = m.group(1)
    method = m.group(2).upper()
    path = m.group(3)

    start = m.end()
    # Find closing fence.
    m_end = re.search(rf"(?m)^{re.escape(fence)}\s*$", md[start:])
    if not m_end:
        # Partial fetch may have truncated before closing fence. We can still proceed with header only.
        yaml_text = md[start:]
    else:
        yaml_text = md[start : start + m_end.start()]

    yaml_lines = yaml_text.splitlines()

    # Find first server url under servers:
    server_url: Optional[str] = None
    for i, line in enumerate(yaml_lines):
        if re.match(r"^\s*servers:\s*$", line):
            for j in range(i + 1, min(i + 80, len(yaml_lines))):
                m_url = re.match(r"^\s*url:\s*(https?://\S+)\s*$", yaml_lines[j])
                if m_url:
                    server_url = m_url.group(1)
                    break
            break

    # Extract first operation-level security block (skip top-level security: []).
    security_lines: List[str] = []
    for i, line in enumerate(yaml_lines):
        if re.match(r"^\s*security:\s*\[\s*\]\s*$", line):
            continue
        if not re.match(r"^\s*security:\s*$", line):
            continue
        indent = _leading_spaces(line)
        collected: List[str] = []
        for j in range(i + 1, len(yaml_lines)):
            nxt = yaml_lines[j]
            if nxt.strip() == "":
                continue
            if _leading_spaces(nxt) <= indent:
                break
            collected.append(nxt.rstrip("\n"))
        if any(re.match(r"^\s*-\s+", x) for x in collected):
            security_lines = collected
            break

    return OpenApiInfo(method=method, path=path, server_url=server_url, security_lines=security_lines)


def _summarize_security(security_lines: List[str]) -> str:
    if not security_lines:
        return "(not found)"
    # Compress YAML-ish indentation to a one-liner.
    # Example lines:
    #   - OAuth2UserToken:
    #       - dm.write
    out: List[str] = []
    current: Optional[str] = None
    scopes: List[str] = []

    def flush() -> None:
        nonlocal current, scopes
        if not current:
            return
        if scopes:
            out.append(f"{current} [{', '.join(scopes)}]")
        else:
            out.append(current)
        current = None
        scopes = []

    for raw in security_lines:
        line = raw.strip()
        m_scheme = re.match(r"^-\s*([A-Za-z0-9_]+):\s*(\[\])?\s*$", line)
        if m_scheme:
            flush()
            current = m_scheme.group(1)
            continue
        m_scope = re.match(r"^-\s*([a-zA-Z0-9_.:-]+)\s*$", line)
        if m_scope and current:
            scopes.append(m_scope.group(1))

    flush()
    return "; ".join(out) if out else "(not found)"


def _auth_env_hint(security_lines: List[str]) -> str:
    s = "\n".join(security_lines)
    if "BearerToken" in s:
        return "$X_BEARER_TOKEN"
    if "OAuth2UserToken" in s or "UserToken" in s:
        return "$X_USER_ACCESS_TOKEN"
    return "$X_BEARER_TOKEN"


def _render_endpoint_block(item: LlmsItem, info: OpenApiInfo) -> str:
    base = info.server_url or "https://api.x.com"
    full = f"{base}{info.path}"
    auth_hint = _auth_env_hint(info.security_lines)
    sec = _summarize_security(info.security_lines)

    method_flag = "" if info.method == "GET" else f"-X {info.method} "
    parts = [f"curl -sS {method_flag}\"{full}\"", f"  -H \"Authorization: Bearer {auth_hint}\"" ]
    if info.method in {"POST", "PUT", "PATCH"}:
        parts.append("  -H \"Content-Type: application/json\"")
        parts.append("  -d '{}'" )
    curl = " \\\n".join(parts)

    return "\n".join(
        [
            f"### {info.method} {info.path}",
            "",
            f"- Doc: `{item.url}`",
            f"- Auth: `{sec}`",
            "",
            "```bash",
            curl,
            "```",
        ]
    )


def _render_autogen_file(
    *,
    group: str,
    llms_url: str,
    group_items: List[LlmsItem],
    openapi_by_url: Dict[str, OpenApiInfo],
) -> str:
    """Render a standalone autogenerated inventory for a single group."""
    open_items = [it for it in group_items if it.url in openapi_by_url]
    non_items = [it for it in group_items if it.url not in openapi_by_url]

    lines: List[str] = []
    lines.append(f"# Autogenerated inventory: x-api/{group}")
    lines.append("")
    lines.append(f"Source: `{llms_url}`")
    lines.append("")

    if open_items:
        lines.append("## Endpoints")
        lines.append("")
        for it in open_items:
            info = openapi_by_url[it.url]
            lines.append(_render_endpoint_block(it, info))
            lines.append("")

    if non_items:
        lines.append("## Docs")
        lines.append("")
        for it in non_items:
            lines.append(_render_doc_link(it))
        lines.append("")

    return "\n".join(lines).rstrip() + "\n"


def _autogen_filename_for_group(group: str) -> str:
    # Some groups in llms.txt are weird singletons like "what-to-build.md".
    # Avoid generating "what-to-build.md.md".
    return group if group.endswith(".md") else f"{group}.md"


def _render_compact_inventory_section(
    *,
    group: str,
    open_items: List[LlmsItem],
    non_items: List[LlmsItem],
    openapi_by_url: Dict[str, OpenApiInfo],
) -> str:
    """Compact, agent-friendly inventory (no curl blocks)."""
    lines: List[str] = []
    lines.append(INVENTORY_HEADING)
    lines.append("")

    if open_items:
        lines.append("### Endpoints")
        lines.append("")
        for it in open_items[:MAX_ENDPOINTS_IN_REFERENCE]:
            info = openapi_by_url[it.url]
            sec = _summarize_security(info.security_lines)
            lines.append(f"- **{info.method} {info.path}** — Auth: `{sec}` — Doc: `{it.url}`")
        if len(open_items) > MAX_ENDPOINTS_IN_REFERENCE:
            lines.append(f"- (and {len(open_items) - MAX_ENDPOINTS_IN_REFERENCE} more endpoints; see `references/x-api-catalog.md`)")
        lines.append("")

    if non_items:
        lines.append("### Docs")
        lines.append("")
        for it in non_items[:MAX_DOC_LINKS_IN_REFERENCE]:
            lines.append(_render_doc_link(it))
        if len(non_items) > MAX_DOC_LINKS_IN_REFERENCE:
            lines.append(f"- (and {len(non_items) - MAX_DOC_LINKS_IN_REFERENCE} more; see `autogen/{_autogen_filename_for_group(group)}`)")
        lines.append("")

    # Do not link to full autogen by default; this repo aims to stay lean for agent installs.
    return "\n".join(lines)


def _upsert_inventory_section(*, path: str, inventory_md: str) -> None:
    """Replace inventory section (from heading to EOF) or append it."""
    if not os.path.exists(path):
        return

    with open(path, "r", encoding="utf-8") as f:
        cur = f.read()
    original = cur

    # Remove legacy "Full inventory" link-only section (from previous iteration).
    # Prefer stripping it up to the Inventory heading (if present), otherwise to EOF.
    cur = re.sub(
        r"(?ms)^## Full inventory \(autogenerated\)\s*\r?\n\s*\r?\nSee `autogen/[^`]+`\.\s*\r?\n\s*\r?\n",
        "",
        cur,
    )
    cur = re.sub(
        r"(?ms)^## Full inventory \(autogenerated\)\s*\r?\n.*?(?=^## Inventory\b)",
        "",
        cur,
    )
    cur = re.sub(
        r"(?ms)^## Full inventory \(autogenerated\)\s*\r?\n.*\Z",
        "",
        cur,
    )

    # Strip boilerplate line(s) if present.
    cur = "\n".join([ln for ln in cur.splitlines() if ln.strip() != CURATED_AUTOGEN_BOILERPLATE]).rstrip() + "\n"

    # If we removed full autogen generation, remove any lingering references to autogen files.
    cur = re.sub(r"(?m)^- \\(and \\d+ more; see `autogen/[^`]+`\\)\\s*$", "", cur)
    cur = cur.replace("autogen/", "")

    if INVENTORY_HEADING in cur:
        pre = cur.split(INVENTORY_HEADING, 1)[0].rstrip()
        out = (pre + "\n\n" if pre else "") + inventory_md.rstrip() + "\n"
    else:
        out = cur.rstrip() + "\n\n" + inventory_md.rstrip() + "\n"

    if out != original:
        with open(path, "w", encoding="utf-8") as f:
            f.write(out)


def _rewrite_reference_file_to_link_inventory(*, path: str, group: str) -> None:
    """Make a reference file lean by removing inlined autogen and linking to autogen/<group>.md."""
    if not os.path.exists(path):
        return

    with open(path, "r", encoding="utf-8") as f:
        cur = f.read()

    post = ""
    pre_raw = cur
    # If we already have an inventory section, strip it so we don't duplicate.
    if INVENTORY_HEADING in pre_raw:
        pre_raw = pre_raw.split(INVENTORY_HEADING, 1)[0]

    # Remove placeholder boilerplate lines.
    pre_lines = [ln for ln in pre_raw.splitlines() if ln.strip() != CURATED_AUTOGEN_BOILERPLATE]
    pre = "\n".join(pre_lines).rstrip()

    link = "\n".join([INVENTORY_HEADING, "", f"{FULL_AUTOGEN_HINT_PREFIX} `autogen/{_autogen_filename_for_group(group)}`", ""])

    out_parts: List[str] = []
    if pre.strip():
        out_parts.append(pre)
        out_parts.append("")  # ensure blank line before link
    out_parts.append(link.rstrip())

    if post.strip():
        out_parts.append("")  # blank line before post content
        out_parts.append(post.rstrip())

    out = "\n".join(out_parts).rstrip() + "\n"

    if out != cur:
        with open(path, "w", encoding="utf-8") as f:
            f.write(out)


def _group_to_target_files(group: str, base_dir: str) -> List[str]:
    base = base_dir
    mapping: Dict[str, List[str]] = {
        "posts": [os.path.join(base, "posts.md")],
        "users": [os.path.join(base, "users.md")],
        "lists": [os.path.join(base, "lists.md")],
        "direct-messages": [os.path.join(base, "direct-messages.md")],
        "spaces": [os.path.join(base, "spaces.md")],
        "trends": [os.path.join(base, "trends.md")],
        "community-notes": [os.path.join(base, "community-notes.md")],
        "communities": [os.path.join(base, "communities.md")],
        "compliance": [os.path.join(base, "compliance.md")],
        "account-activity": [os.path.join(base, "account-activity.md")],
        "activity": [os.path.join(base, "activity.md")],
        "connections": [os.path.join(base, "connections.md")],
        "stream": [os.path.join(base, "streams.md")],
        "media": [os.path.join(base, "media.md")],
        "webhooks": [os.path.join(base, "webhooks.md")],
        "news": [os.path.join(base, "news.md")],
        "usage": [os.path.join(base, "usage.md")],
        "migrate": [os.path.join(base, "migrate.md")],
        "powerstream": [os.path.join(base, "powerstream.md")],
        "enterprise-gnip-2.0": [os.path.join(base, "enterprise-gnip-2.0.md")],
        "getting-started": [os.path.join(base, "getting-started.md")],
        "fundamentals": [os.path.join(base, "api-fundamentals.md")],
        "tools-and-libraries": [os.path.join(base, "tools-and-libraries.md")],
        "bookmarks": [os.path.join(base, "bookmarks.md")],
        # Weird singleton groups in llms.txt: fold into getting-started.
        "introduction.md": [os.path.join(base, "getting-started.md")],
        "what-to-build.md": [os.path.join(base, "getting-started.md")],
    }

    return mapping.get(group, [os.path.join(base, f"x-api-{group}.md")])


def main(argv: List[str]) -> int:
    ap = argparse.ArgumentParser()
    ap.add_argument("--llms-url", default=LLMS_DEFAULT)
    ap.add_argument("--out-dir", default=DEFAULT_OUT_DIR)
    ap.add_argument(
        "--write-full-autogen",
        action="store_true",
        help="Write full inventories with curl skeletons to references/autogen/. Off by default for token-efficient installs.",
    )
    args = ap.parse_args(argv[1:])

    llms_text = _read_url(args.llms_url, max_bytes=2 * 1024 * 1024)
    items = parse_llms(llms_text)

    x_api_items = [it for it in items if it.url.startswith("https://docs.x.com/x-api/")]

    # Group by x-api/<group>
    grouped: Dict[str, List[LlmsItem]] = defaultdict(list)
    for it in x_api_items:
        path = it.url.split("https://docs.x.com/", 1)[1]
        parts = path.split("/")
        group = parts[1] if len(parts) > 1 else ""
        # Normalize weird singleton groups into getting-started.
        if group in {"introduction.md", "what-to-build.md"}:
            group = "getting-started"
        grouped[group].append(it)

    group_counts = Counter({k: len(v) for k, v in grouped.items()})

    # Fetch + extract openapi concurrently.
    openapi_by_url: Dict[str, OpenApiInfo] = {}
    errors: List[Tuple[str, str]] = []

    def worker(it: LlmsItem) -> Tuple[str, Optional[OpenApiInfo], Optional[str]]:
        try:
            md = _read_url(it.url, FETCH_MAX_BYTES)
            info = try_extract_openapi(md)
            return (it.url, info, None)
        except Exception as e:
            return (it.url, None, str(e))

    start = time.time()
    with concurrent.futures.ThreadPoolExecutor(max_workers=WORKERS) as ex:
        futs = [ex.submit(worker, it) for it in x_api_items]
        for fut in concurrent.futures.as_completed(futs):
            url, info, err = fut.result()
            if err:
                errors.append((url, err))
                continue
            if info:
                openapi_by_url[url] = info

    elapsed = time.time() - start

    out_dir = os.path.abspath(args.out_dir)
    os.makedirs(out_dir, exist_ok=True)
    autogen_dir = os.path.join(out_dir, AUTOGEN_DIRNAME)
    if args.write_full_autogen:
        os.makedirs(autogen_dir, exist_ok=True)
        for name in os.listdir(autogen_dir):
            if name.endswith(".md"):
                os.remove(os.path.join(autogen_dir, name))

    # Write groups overview.
    groups_path = os.path.join(out_dir, "x-api-groups.md")
    with open(groups_path, "w", encoding="utf-8") as f:
        f.write("# X API Docs Map\n\n")
        f.write(f"Generated from `{args.llms_url}`.\n\n")
        f.write("This is a navigation aid for the curated `references/*.md` files.\n")
        f.write("Each curated reference ends with a compact `## Inventory` section (agent-friendly).\n")
        f.write("Full inventories with curl skeletons are optional (generated on demand).\n\n")

        f.write("## Group reference files\n\n")
        for group, _count in group_counts.most_common():
            targets = _group_to_target_files(group, out_dir)
            for t in targets:
                rel = os.path.relpath(t, start=out_dir)
                f.write(f"- `{group}` -> `{rel}`\n")
        f.write("\n")

        if args.write_full_autogen:
            f.write("## Full autogenerated inventories (optional)\n\n")
            f.write("These are larger files (include curl skeletons). They are generated only when requested.\n\n")
            for group, _count in group_counts.most_common():
                f.write(f"- `{group}` -> `{AUTOGEN_DIRNAME}/{_autogen_filename_for_group(group)}`\n")
            f.write("\n")

    # Write catalog.
    catalog_path = os.path.join(out_dir, "x-api-catalog.md")
    with open(catalog_path, "w", encoding="utf-8") as f:
        f.write("# X API Catalog (Comprehensive)\n\n")
        f.write("This file enumerates every `x-api/*` page from `docs.x.com/llms.txt`.\n\n")
        f.write("- For OpenAPI pages, we extract `METHOD PATH`, security schemes/scopes, and generate a curl skeleton.\n")
        f.write("- For non-OpenAPI pages (integration guides, quickstarts, intros), we list the link under its group.\n\n")
        f.write("Use ripgrep to find anything quickly:\n")
        f.write("```bash\n")
        f.write("rg -n \"/2/|OAuth2UserToken|BearerToken|tweet.write|dm.write|media\" references/x-api-catalog.md\n")
        f.write("```\n\n")

        for group in sorted(grouped.keys(), key=lambda g: (-group_counts[g], g)):
            f.write(f"## {group}\n\n")
            group_items = sorted(grouped[group], key=lambda it: it.url)

            # OpenAPI first, then non-OpenAPI links.
            open_items = [it for it in group_items if it.url in openapi_by_url]
            non_items = [it for it in group_items if it.url not in openapi_by_url]

            if open_items:
                f.write("### Endpoints\n\n")
                for it in open_items:
                    info = openapi_by_url[it.url]
                    f.write(_render_endpoint_block(it, info))
                    f.write("\n\n")

            if non_items:
                f.write("### Docs\n\n")
                for it in non_items:
                    f.write(_render_doc_link(it) + "\n")
                f.write("\n")

    # Optionally write per-group full inventories (standalone files with curl skeletons).
    if args.write_full_autogen:
        for group, items_in_group in grouped.items():
            group_items = sorted(items_in_group, key=lambda it: it.url)
            autogen_md = _render_autogen_file(
                group=group,
                llms_url=args.llms_url,
                group_items=group_items,
                openapi_by_url=openapi_by_url,
            )

            autogen_path = os.path.join(autogen_dir, _autogen_filename_for_group(group))
            with open(autogen_path, "w", encoding="utf-8") as f:
                f.write(autogen_md)

    # Upsert compact in-file inventory sections (agent-friendly).
    for group, items_in_group in grouped.items():
        group_items = sorted(items_in_group, key=lambda it: it.url)
        open_items = [it for it in group_items if it.url in openapi_by_url]
        non_items = [it for it in group_items if it.url not in openapi_by_url]
        inventory_md = _render_compact_inventory_section(
            group=group,
            open_items=open_items,
            non_items=non_items,
            openapi_by_url=openapi_by_url,
        )
        for target in _group_to_target_files(group, out_dir):
            _upsert_inventory_section(path=target, inventory_md=inventory_md)

    return 0


if __name__ == "__main__":
    raise SystemExit(main(sys.argv))
